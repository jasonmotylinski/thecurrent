{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data import load, get_training_data\n",
    "\n",
    "load(sqlContext, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splits = df.randomSplit([0.8, 0.2])\n",
    "train = splits[0].cache()\n",
    "test = splits[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "artistInd = StringIndexer(inputCol=\"artist\", outputCol=\"artistIndex\").setHandleInvalid(\"skip\")\n",
    "titleInd = StringIndexer(inputCol=\"title\", outputCol=\"titleIndex\").setHandleInvalid(\"skip\")\n",
    "dayOfWeekInd = StringIndexer(inputCol=\"day_of_week\", outputCol=\"dayOfWeekIndex\").setHandleInvalid(\"skip\")\n",
    "assembler = VectorAssembler(inputCols=[\"artistIndex\", \"dayOfWeekIndex\", \"titleIndex\", \"hour\", \"day\", \"month\", \"year\",\"currentrank\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, threshold=0.1)\n",
    "pipeline = Pipeline().setStages([artistInd, titleInd, dayOfWeekInd, assembler, lr])\n",
    "\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "results = model.transform(test)\n",
    "predictionsAndLabels = results.select(\"prediction\", \"label\") \\\n",
    "                              .rdd.map(lambda r: (float(r[\"prediction\"]), float(r[\"label\"])))\n",
    "metrics = BinaryClassificationMetrics(predictionsAndLabels)\n",
    "print \"AUC: {0}\".format(metrics.areaUnderROC)\n",
    "print \"AUP: {0}\".format(metrics.areaUnderPR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "results.filter(col(\"prediction\") == 1.0).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data import load, get_training_data\n",
    "from datetime import datetime\n",
    "load(sqlContext, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+-----+----+-----------------+--------+-----------+-----+\n",
      "|              artist|               title|year|month|week|weekly_play_count|nextrank|currentrank|label|\n",
      "+--------------------+--------------------+----+-----+----+-----------------+--------+-----------+-----+\n",
      "|             temples|strange or be for...|2017|    3|   9|               11|       0|          0|    0|\n",
      "|         jens lekman|      evening prayer|2017|    3|   9|               11|       0|          0|    0|\n",
      "|         marian hill|                down|2017|    3|   9|               10|      16|         18|    1|\n",
      "|         mac demarco|          my old man|2017|    3|   9|               10|       0|          0|    0|\n",
      "|           tim darcy| tall glass of water|2017|    3|   9|               10|       0|          0|    0|\n",
      "|       the lumineers|              angela|2017|    3|   9|               10|       0|          0|    0|\n",
      "|    guided by voices|         hiking skin|2017|    3|   9|                9|       0|          0|    0|\n",
      "|         real estate|             darling|2017|    3|   9|                9|       0|          0|    0|\n",
      "|the new pornograp...|high ticket attra...|2017|    3|   9|                9|       0|         20|    0|\n",
      "|                jain|              makeba|2017|    3|   9|                8|       0|          0|    0|\n",
      "|       cold war kids|    love is mystical|2017|    3|   9|                8|       0|          0|    0|\n",
      "|     dead man winter|           destroyer|2017|    3|   9|                8|       1|          2|    1|\n",
      "|    childish gambino|             redbone|2017|    3|   9|                8|       0|          0|    0|\n",
      "|         john legend|darkness and ligh...|2017|    3|   9|                8|       0|          0|    0|\n",
      "|         arcade fire|i give you power ...|2017|    3|   9|                8|      10|          0|    1|\n",
      "|         haley bonar|         stupid face|2017|    3|   9|                8|       5|          7|    1|\n",
      "|      rag'n'bone man|               human|2017|    3|   9|                8|      19|          0|    1|\n",
      "|  the japanese house|   face like thunder|2017|    3|   9|                7|       0|          0|    0|\n",
      "|      regina spektor|    older and taller|2017|    3|   9|                7|       0|          0|    0|\n",
      "|     angelica garcia|       orange flower|2017|    3|   9|                7|       0|          0|    0|\n",
      "+--------------------+--------------------+----+-----+----+-----------------+--------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = get_training_data(spark, datetime(2017,3,1))\n",
    "df = df.unionAll(get_training_data(spark, datetime(2017,2,22))) \\\n",
    "       .unionAll(get_training_data(spark, datetime(2017,2,15))) \\\n",
    "      .unionAll(get_training_data(spark, datetime(2017,2,8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data import get_training_data   \n",
    "df = get_training_data(spark, datetime(2017,3,1))\n",
    "df = df.unionAll(get_training_data(spark, datetime(2017,2,22))) \\\n",
    "  .unionAll(get_training_data(spark, datetime(2017,2,15))) \\\n",
    "  .unionAll(get_training_data(spark, datetime(2017,2,8))) \\\n",
    "  .unionAll(get_training_data(spark, datetime(2017,2,1)))\n",
    "df.dropna()\n",
    "splits = df.randomSplit([0.8, 0.2])\n",
    "train = splits[0].cache()\n",
    "test = splits[1].cache()\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "artistInd = StringIndexer(inputCol=\"artist\", outputCol=\"artistIndex\").setHandleInvalid(\"skip\")\n",
    "titleInd = StringIndexer(inputCol=\"title\", outputCol=\"titleIndex\").setHandleInvalid(\"skip\")\n",
    "assembler = VectorAssembler(inputCols=[\"artistIndex\",\"titleIndex\", \"weekly_play_count\", \"week\", \"month\", \"year\", \"currentrank\"], outputCol=\"features\")\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor,LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "linear = LinearRegression(maxIter=1000)\n",
    "lr = LogisticRegression(maxIter=1000)\n",
    "pipeline = Pipeline().setStages([artistInd, titleInd, assembler])\n",
    "\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = artistInd.fit(train).transform(train)\n",
    "d = titleInd.fit(d).transform(d)\n",
    "d = assembler.transform(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artistIndex</th>\n",
       "      <th>titleIndex</th>\n",
       "      <th>weekly_play_count</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>currentrank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.378052</td>\n",
       "      <td>-0.171458</td>\n",
       "      <td>0.130355</td>\n",
       "      <td>1.418347e-01</td>\n",
       "      <td>-7.109485e-08</td>\n",
       "      <td>-0.101029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.780523e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.345547</td>\n",
       "      <td>0.299494</td>\n",
       "      <td>3.248992e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.154544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.714579e-01</td>\n",
       "      <td>-0.345547</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015871</td>\n",
       "      <td>-4.861671e-03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.368848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.303549e-01</td>\n",
       "      <td>0.299494</td>\n",
       "      <td>-0.015871</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.578186e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.418347e-01</td>\n",
       "      <td>0.324899</td>\n",
       "      <td>-0.004862</td>\n",
       "      <td>0.957819</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.027276e-07</td>\n",
       "      <td>-0.007906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-7.109485e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.027276e-07</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.010292e-01</td>\n",
       "      <td>-0.154544</td>\n",
       "      <td>0.368848</td>\n",
       "      <td>-0.000101</td>\n",
       "      <td>-7.905787e-03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    artistIndex  titleIndex  weekly_play_count      week         month  \\\n",
       "0  1.000000e+00    0.378052          -0.171458  0.130355  1.418347e-01   \n",
       "1  3.780523e-01    1.000000          -0.345547  0.299494  3.248992e-01   \n",
       "2 -1.714579e-01   -0.345547           1.000000 -0.015871 -4.861671e-03   \n",
       "3  1.303549e-01    0.299494          -0.015871  1.000000  9.578186e-01   \n",
       "4  1.418347e-01    0.324899          -0.004862  0.957819  1.000000e+00   \n",
       "5 -7.109485e-08    0.000000           0.000000  0.000000 -1.027276e-07   \n",
       "6 -1.010292e-01   -0.154544           0.368848 -0.000101 -7.905787e-03   \n",
       "\n",
       "           year  currentrank  \n",
       "0 -7.109485e-08    -0.101029  \n",
       "1  0.000000e+00    -0.154544  \n",
       "2  0.000000e+00     0.368848  \n",
       "3  0.000000e+00    -0.000101  \n",
       "4 -1.027276e-07    -0.007906  \n",
       "5  1.000000e+00     0.000000  \n",
       "6  0.000000e+00     1.000000  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-7d121904761a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"probability\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"artist\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"probability\"\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "results.filter(col(\"prediction\") == 1.0) \\\n",
    "       .sort(col(\"probability\"), ascending=False) \\\n",
    "       .select(\"artist\",\"prediction\", \"probability\") \\\n",
    "       .take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = predict(model, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = get_data(datetime(2017,3,8))\n",
    "results = predict(model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\n",
    "\"\"\"\n",
    "SELECT \n",
    "    p.artist,\n",
    "    p.title,\n",
    "    p.year,\n",
    "    p.month,\n",
    "    p.day,\n",
    "    p.day_of_week,\n",
    "    p.week,\n",
    "    COUNT(*) AS daily_play_count\n",
    "FROM playlist p\n",
    "GROUP BY\n",
    "    p.artist,\n",
    "    p.title,\n",
    "    p.year,\n",
    "    p.month,\n",
    "    p.day,\n",
    "    p.day_of_week,\n",
    "    p.week\n",
    "ORDER BY \n",
    "    COUNT(*) DESC\n",
    "\"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
